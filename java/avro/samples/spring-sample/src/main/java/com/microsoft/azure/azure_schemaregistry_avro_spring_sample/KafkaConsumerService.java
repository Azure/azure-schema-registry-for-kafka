package com.microsoft.azure.azure_schemaregistry_avro_spring_sample;

import com.microsoft.azure.azure_schemaregistry_avro_spring_sample.model.Customer;
import com.microsoft.azure.schemaregistry.kafka.avro.KafkaAvroDeserializer;
import com.microsoft.azure.schemaregistry.kafka.avro.KafkaAvroDeserializerConfig;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.stereotype.Service;

import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import java.time.Duration;
import java.util.Collections;
import java.util.Properties;
import java.util.concurrent.atomic.AtomicBoolean;

@Service
public class KafkaConsumerService {
	private static final Logger logger = LoggerFactory.getLogger(KafkaConsumerService.class);

	private final AppConfig appConfig;
	private KafkaConsumer<String, Customer> consumer;
	private final AtomicBoolean running = new AtomicBoolean(false);
	private Thread consumerThread;

	public KafkaConsumerService(AppConfig appConfig) {
		this.appConfig = appConfig;
	}

	@PostConstruct
	public void init() {
		Properties props = new Properties();

		// Basic Kafka consumer settings
		props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, appConfig.getBootstrapServers());
		props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class);
		props.put(ConsumerConfig.GROUP_ID_CONFIG, appConfig.getGroupId());
		props.put(ConsumerConfig.CLIENT_ID_CONFIG, "customer-consumer");

		// Azure Schema Registry specific settings
		props.put("schema.registry.url", appConfig.getSchemaRegistryEndpoint());
		props.put("schema.group", appConfig.getSchemaGroup());

		// Authentication for Schema Registry
		// Using this value will use the DefaultAzureCredential generated by the Kafka
		// Avro Library
		props.put("use.azure.credential", appConfig.getUseAzureCredential());

		// Specify that we want the deserializer to return specific Avro record types
		props.put(KafkaAvroDeserializerConfig.AVRO_SPECIFIC_READER_CONFIG, true);
		props.put(KafkaAvroDeserializerConfig.AVRO_SPECIFIC_VALUE_TYPE_CONFIG, Customer.class);

		// Consumer behavior settings
		props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
		props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");

		// Connection settings for Azure Event Hubs
		props.put("security.protocol", "SASL_SSL");
		props.put("sasl.mechanism", "PLAIN");
		props.put("sasl.jaas.config", appConfig.buildJaasConfig());

		// Create the Kafka consumer
		this.consumer = new KafkaConsumer<>(props);

		logger.info("Kafka consumer initialized with bootstrap servers: {}", appConfig.getBootstrapServers());
	}

	/**
	 * Starts consuming messages in a separate thread
	 */
	public void startConsuming() {
		if (running.compareAndSet(false, true)) {
			consumer.subscribe(Collections.singletonList(appConfig.getTopicName()));
			logger.info("Subscribed to topic: {}", appConfig.getTopicName());

			consumerThread = new Thread(this::consumeMessages);
			consumerThread.start();
		}
	}

	/**
	 * Stops consuming messages
	 */
	public void stopConsuming() {
		if (running.compareAndSet(true, false)) {
			if (consumerThread != null) {
				consumerThread.interrupt();
			}
		}
	}

	/**
	 * Consumer loop that processes messages until stopConsuming() is called
	 * Now receiving Customer objects directly, deserialization handled by
	 * KafkaAvroDeserializer
	 */
	private void consumeMessages() {
		try {
			while (running.get()) {
				ConsumerRecords<String, Customer> records = consumer.poll(Duration.ofMillis(1000));

				for (ConsumerRecord<String, Customer> record : records) {
					try {
						// Get the Customer object directly from the record
						Customer customer = record.value();

						// Process the customer record
						logger.info("Received customer: {} with key: {} from partition: {}, offset: {}",
								customer, record.key(), record.partition(), record.offset());

						// Display the received customer info
						System.out.println("------- Received Customer -------");
						System.out.println("ID: " + customer.getId());
						System.out.println("Name: " + customer.getName());
						System.out.println("Email: " + customer.getEmail());
						System.out.println("Registration Date: " + customer.getRegistrationDate());
						System.out.println("Active: " + customer.getActive());
						System.out.println("--------------------------------");

					} catch (Exception e) {
						logger.error("Error processing record: {}", e.getMessage(), e);
					}
				}

				// Commit offsets manually after processing
				if (!records.isEmpty()) {
					consumer.commitSync();
				}
			}
		} catch (Exception e) {
			if (!Thread.currentThread().isInterrupted()) {
				logger.error("Error in consumer thread: {}", e.getMessage(), e);
			}
		} finally {
			consumer.close();
			logger.info("Consumer closed");
		}
	}

	@PreDestroy
	public void close() {
		stopConsuming();
	}
}